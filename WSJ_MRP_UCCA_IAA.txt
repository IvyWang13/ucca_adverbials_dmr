Cohen's Kappa for each category: {'aspectual': 0.8091603053435115, 'condition': 1.0, 'degree': 0.5061728395061729, 'description': 0.7183098591549295, 'quantity': 0.5815899581589958, 'comparison': 0.5815899581589958, 'possibility': 0.9342105263157895, 'support': 0.6610169491525424, 'negation': 1.0}
Cohen's Kappa averaged across categories: 0.7546722661989931 

--------partial correct examples via accuracy, treating #2 as the gold----
Partial: avg_accuracy=0.7937500000000001 
avg_prec=, 0.7937500000000001
avg_recall= 0.8
exact match= 0.7875

Multilabel Confusion Matrix, Label-based:
Labels:  ['aspectual', 'condition', 'degree', 'description', 'quantity', 'comparison', 'possibility', 'support', 'negation']  
For what the coordinates represent see README

  [[[61  2]
  [ 3 14]]

 [[79  0]
  [ 0  1]]

 [[67  1]
  [ 7  5]]

 [[49  9]
  [ 1 21]]

 [[71  3]
  [ 2  4]]

 [[71  2]
  [ 3  4]]

 [[71  0]
  [ 1  8]]

 [[78  1]
  [ 0  1]]

 [[69  0]
  [ 0 11]]]

Confusion Matrix
  [[14  0  0  1  0  0  0  2  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  1  0  0  0  0  3  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0  0]
 [ 0  1  0  0  0  0  3  2  0  0  0  0  3  0  0]
 [ 1  0  0  0  0  0  0 17  0  0  0  0  0  0  0]
 [ 0  0  0  0  3  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0 11  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  8  0  0  1]
 [ 0  0  0  0  0  0  0  2  0  0  0  0  2  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  2  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]]
By label:
mcm accuracy -[0.9375 1.     0.9    0.875  0.9375 0.9375 0.9875 0.9875 1.    ]
mcm precision - [0.875      1.         0.83333333 0.7        0.57142857 0.66666667
 1.         0.5        1.        ]
mcm recall -  [0.82352941 1.         0.41666667 0.95454545 0.66666667 0.57142857
 0.88888889 1.         1.        ]

Classification report
              precision    recall  f1-score   support

   aspectual       0.88      0.82      0.85        17
   condition       1.00      1.00      1.00         1
      degree       0.83      0.42      0.56        12
 description       0.70      0.95      0.81        22
    quantity       0.57      0.67      0.62         6
  comparison       0.67      0.57      0.62         7
 possibility       1.00      0.89      0.94         9
     support       0.50      1.00      0.67         1
    negation       1.00      1.00      1.00        11

   micro avg       0.79      0.80      0.80        86
   macro avg       0.79      0.81      0.78        86
weighted avg       0.81      0.80      0.79        86
 samples avg       0.80      0.79      0.80        86
